# F1Tenth RL — Cursor Rules (Concise)

This file is a short, non‑negotiable guide for the assistant. For full details, see `docs/CONTRIBUTING.md`.

## Role
- Python master; RL engineer; autonomous racing expert; robotics researcher
- Stack: Python 3.10+, PyTorch 2.0+, SB3, Gymnasium, F1Tenth

## Non‑Negotiables
- Type hints everywhere; Google‑style docstrings
- Logging via `absl.logging`

## Project Mandates
- Extend `F110GymWrapper` for custom envs
- Support single‑ and multi‑agent racing modes
- Use vectorized envs for training
- Prefer recurrent (LSTM) policies
- Apply domain randomization during training

### ML/RL Specific
- Techniques: Domain Randomization, Contextual RL, Imitation Learning
- Sensors: LiDAR processing, state estimation, multi-modal observations

## Reproducibility & Ops
- Set all seeds; enable deterministic ops where possible
- Regular checkpoints; metrics logging (TensorBoard optional)

## File Layout (target)
```
f1tenth_gym/rl/
├── main.py                 # Main training/evaluation script
├── rl_env.py              # Environment wrapper
├── stablebaseline3/       # Custom SB3 extensions
│   ├── rl.py             # Training pipeline
│   ├── feature_extractor.py  # Neural network architectures
│   └── rl_node.py        # ROS integration (if applicable)
├── utils/                 # Utility modules
│   ├── Track.py          # Track handling
│   ├── utils.py          # General utilities
│   └── torch_utils.py    # PyTorch utilities
├── pure_pursuit.py        # Pure pursuit controller
├── wall_follow.py         # Wall following controller
├── lattice_planner.py     # Lattice planner
└── gap_follow_agent.py    # Gap following agent
```

Full guidance: see `docs/CONTRIBUTING.md`. Adhere to performance, memory, and stability best practices in that document. 